{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "Suposiciones:\n",
    "\n",
    "$X_i$ observación i.  \n",
    "$y_i=k | k \\in \\{0,1\\}$ clase a la que pertenece el vector $X_i$.\n",
    "\n",
    "$$p(X_i|y_i=k) \\sim \\mathcal{N}(\\mu_k,\\,\\Sigma)$$\n",
    "\n",
    "es decir:\n",
    "\n",
    "$$p(X_i|y_i)=\n",
    "\\frac\n",
    " {1}\n",
    " {(2\\pi)^{D/2}|\\Sigma|^{1/2}}\n",
    "\\exp\n",
    "\\left(\n",
    " -\\frac{1}{2}\n",
    " ( X_i - \\vec \\mu_k)^\\top \\Sigma^{-1} (X_i - \\vec \\mu_k)\n",
    "\\right)$$  \n",
    "\n",
    "$\\vec \\mu_k$: vector de media de las observaciones $X_i$ que pertenecen a la clase $k$.  \n",
    "$\\Sigma$: Matriz de covarianza de las observaciones X_i. Suponemos que todas las distribuciones Gaussianas tienen la misma matriz de covarianza. Es decir $\\Sigma=\\Sigma_0=\\Sigma_1$. A esta condición se la llama **homocedasticidad**.\n",
    "\n",
    "Teniendo en cuenta estas suposiciones, definimos el clasificador Bayesiano:\n",
    "\n",
    "$$P(y_i=k|X_i)=\\frac{p(X_i|y_i=k)P(y_i=k)}{p(X_i)}$$\n",
    "\n",
    "Donde $P(y_i=k)=\\pi_k$.\n",
    "\n",
    "Para clasificar, vamos a utilizar el logodds:\n",
    "\n",
    "$$Lodds=\\ln \\frac{P(y_i=1|X_i)}{P(y_i=0|X_i)}$$\n",
    "\n",
    "Si $Lodds$ >0, entonces $\\hat y_i=1$.  \n",
    "Si $Lodds$ <0, entonces $\\hat y_i=0$.\n",
    "\n",
    "Nos interesa estudiar el umbral en el que cambia la condición de clasificación, es decir: $Lodds=0$. Es decir, reemplazando las distribuciones condicionales nos queda que el umbral que buscamos está dado por la condición:\n",
    "\n",
    "$$(X_i- \\vec \\mu_0)^T \\Sigma_0^{-1} ( X_i- \\vec \\mu_0) + \\ln|\\Sigma_0| - (X_i- \\vec \\mu_1)^T \\Sigma_1^{-1} ( X_i- \\vec \\mu_1) - \\ln|\\Sigma_1| - \\ln \\pi_1 + \\ln \\pi_0 =0$$\n",
    "\n",
    "Si desarrollamos utilizando la condición de homocedasticidad:\n",
    "\n",
    "$$X_i^T \\Sigma_0^{-1} X_i = X_i^T \\Sigma_1^{-1} X_i$$.\n",
    "\n",
    "Y además tenemos en cuenta que:\n",
    "\n",
    "$X_i^T {\\Sigma}^{-1} \\vec{\\mu_k} = {\\vec{\\mu_k}}^T{\\Sigma}^{-1} X_i$ ya que  $\\Sigma_i$ es simétrica. (Se puede demostrar utilizando la propiedad $\\vec v^T A= (A^T \\vec v)^T$ aplicada dos veces.)  \n",
    "\n",
    "Nos queda que el umbral de decisión queda reducido a:\n",
    "\n",
    "$$\\vec w  X_i - c = 0$$\n",
    "\n",
    "Donde:  \n",
    "\n",
    "$$\\vec w = \\Sigma^{-1} (\\vec \\mu_1 - \\vec \\mu_0)$$\n",
    "$$ c = \\frac{1}{2}(\\ln\\pi_1-\\ln\\pi_0-{\\vec{\\mu_0}}^T \\Sigma_0^{-1} {\\vec{\\mu_0}}+{\\vec{\\mu_1}}^T \\Sigma_1^{-1} {\\vec{\\mu_1}})$$\n",
    "\n",
    "\n",
    "**Por lo tanto el umbral de decisión es un hiperplano y el criterio de decisión depende linealmente de X**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio entregable:\n",
    "\n",
    "Para un problema con datos bidimensionales (D=2), demostrar que el umbral de decisión en el caso de que no se cumpla la condición de homocedasticidad es cuadrático. A esta herramienta se la conoce como **QDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (borrar)",
   "language": "python",
   "name": "borrar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
